# Web Scraping y AnÃ¡lisis de Series de TV

ImplementaciÃ³n de un sistema de scraping y anÃ¡lisis de datos sobre series de televisiÃ³n de la pagina wed SensaCine, utilizando Python y pandas. El proyecto permite extraer informaciÃ³n de la web, limpiar y fusionar datos, y realizar anÃ¡lisis exploratorio y visualizaciones.

## ğŸ‘¤ Autor

| Nombre | GitHub | MatrÃ­cula |
|--------|--------|-----------|
| Antonio Jesus Benavides Puentes | [@AntoCreed777](https://github.com/AntoCreed777) | 2023455954 |

## ğŸ“‹ Tabla de Contenidos
- [Web Scraping y AnÃ¡lisis de Series de TV](#web-scraping-y-anÃ¡lisis-de-series-de-tv)
  - [ğŸ‘¤ Autor](#-autor)
  - [ğŸ“‹ Tabla de Contenidos](#-tabla-de-contenidos)
  - [ğŸ› ï¸ TecnologÃ­as Utilizadas](#ï¸-tecnologÃ­as-utilizadas)
    - [Lenguaje de programaciÃ³n](#lenguaje-de-programaciÃ³n)
    - [Herramientas de desarrollo y control de versiones](#herramientas-de-desarrollo-y-control-de-versiones)
    - [Testing, Linting y Tipado](#testing-linting-y-tipado)
    - [IntegraciÃ³n continua](#integraciÃ³n-continua)
  - [ğŸ“‹ Requisitos Previos](#-requisitos-previos)
  - [ğŸš€ CÃ³mo ejecutar y probar el proyecto](#-cÃ³mo-ejecutar-y-probar-el-proyecto)
    - [1. Instalar pdm (si no lo tienes)](#1-instalar-pdm-si-no-lo-tienes)
    - [2. Instalar dependencias](#2-instalar-dependencias)
    - [3. Ejecutar el scraping](#3-ejecutar-el-scraping)
    - [4. Ejecutar el anÃ¡lisis](#4-ejecutar-el-anÃ¡lisis)
    - [5. Linting y tipado](#5-linting-y-tipado)
    - [6. Fusionar archivos descargados](#6-fusionar-archivos-descargados)
  - [ğŸ—‚ï¸ Estructura del Proyecto](#ï¸-estructura-del-proyecto)
  - [ğŸ“š DocumentaciÃ³n del CÃ³digo](#-documentaciÃ³n-del-cÃ³digo)
  - [ğŸ““ Jupyter Notebook](#-jupyter-notebook)
  - [ğŸ“¥ Descarga por rangos de pÃ¡ginas y fusiÃ³n de datos](#-descarga-por-rangos-de-pÃ¡ginas-y-fusiÃ³n-de-datos)
  - [ğŸ“ Convenciones y Herramientas de Formato](#-convenciones-y-herramientas-de-formato)

## ğŸ› ï¸ TecnologÃ­as Utilizadas

<div align="center">

### Lenguaje de programaciÃ³n
<img src="https://skillicons.dev/icons?i=python&perline=8" />

### Herramientas de desarrollo y control de versiones
<img src="https://skillicons.dev/icons?i=git,github,vscode&perline=5" />

### Testing, Linting y Tipado
<img src="https://img.shields.io/badge/pytest-0A9EDC?style=for-the-badge&logo=pytest&logoColor=white" />
<img src="https://img.shields.io/badge/flake8-4B8BBE?style=for-the-badge&logo=python&logoColor=white" />
<img src="https://img.shields.io/badge/mypy-2A6DB2?style=for-the-badge&logo=python&logoColor=white" />

### IntegraciÃ³n continua
<img src="https://skillicons.dev/icons?i=githubactions&perline=8" />

</div>

## âš ï¸ Aviso Legal

Este repositorio fue creado con fines educativos como parte de una tarea universitaria.  
El cÃ³digo de scraping incluido estÃ¡ diseÃ±ado Ãºnicamente para propÃ³sitos de aprendizaje y no debe utilizarse para recopilar datos de forma masiva o sin consentimiento.

Por favor, asegÃºrate de respetar los TÃ©rminos de Servicio del sitio web que se scrapea y revisa su archivo `robots.txt` antes de ejecutar este script.

El autor no se hace responsable del mal uso de este cÃ³digo.

## ğŸ“‹ Requisitos Previos

- **Sistema operativo:** Linux, macOS, o Windows con WSL
- **Python 3.8+**
- **pdm** para gestiÃ³n de dependencias y entornos

## ğŸš€ CÃ³mo ejecutar y probar el proyecto

### 1. Instalar pdm (si no lo tienes)

```bash
pip install pdm
```

### 2. Instalar dependencias

```bash
git clone https://github.com/AntoCreed777/Web_Scraping
cd Web_Scraping
pdm install
```

### 3. Ejecutar el scraping

```bash
pdm run python src/scraping/main.py
```

### 4. Ejecutar el anÃ¡lisis

```bash
pdm run python src/analisis/main.py
```

### 5. Linting y tipado

Para revisar el estilo de cÃ³digo:

```bash
pdm run flake8 src/
```

Para revisar el tipado:

```bash
pdm run mypy src/
```

### 6. Fusionar archivos descargados

Para fusionar los archivos de series descargados por rangos de pÃ¡ginas y obtener un Ãºnico archivo consolidado, ejecuta:

```bash
pdm run python src/scraping/fusion_pkl.py
```

> [!NOTE]
> La fusiÃ³n de archivos descargados permite consolidar todos los datos obtenidos en diferentes rangos de pÃ¡ginas en un solo archivo. Esto es especialmente Ãºtil cuando la web bloquea descargas masivas, ya que puedes descargar por partes y luego unir todo para el anÃ¡lisis final. Consulta la secciÃ³n [Descarga por rangos de pÃ¡ginas y fusiÃ³n de datos](#-descarga-por-rangos-de-pÃ¡ginas-y-fusiÃ³n-de-datos) para mÃ¡s detalles sobre cÃ³mo funciona este proceso.

## ğŸ—‚ï¸ Estructura del Proyecto

```
Web_Scraping/
â”œâ”€â”€ 2025_Tarea01_AD.ipynb
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraping/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ extraer_datos.py
â”‚   â”‚   â”œâ”€â”€ datos_serie.py
â”‚   â”‚   â”œâ”€â”€ fusion_pkl.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ analisis/
â”‚       â”œâ”€â”€ main.py
â”‚       â””â”€â”€ utilities.py
â””â”€â”€ ...
```

## ğŸ“š DocumentaciÃ³n del CÃ³digo

La documentaciÃ³n se encuentra en los docstrings de las funciones y clases principales. Para explorar la lÃ³gica y el flujo de datos, revisa los archivos en `src/scraping/` y `src/analisis/`.

## ğŸ““ Jupyter Notebook

El proyecto incluye un Jupyter Notebook que sirve como documento de entrega y facilita la exploraciÃ³n, anÃ¡lisis y visualizaciÃ³n de los datos obtenidos mediante scraping. En el notebook se explican los pasos principales del proceso, se muestran ejemplos de anÃ¡lisis y se interpretan los resultados. Es ideal para presentar el trabajo de forma ordenada y reproducible.

## ğŸ“¥ Descarga por rangos de pÃ¡ginas y fusiÃ³n de datos

Para evitar bloqueos al descargar grandes volÃºmenes de datos desde la web, el sistema permite realizar descargas parciales de rangos de pÃ¡ginas. Cada rango se guarda en un archivo independiente en formato `.pkl` (pickle de pandas), siguiendo el nombre `series_tv_<desde>_<hasta>.pkl`, donde `<desde>` y `<hasta>` corresponden al rango de pÃ¡ginas descargado.

Posteriormente, el proyecto incluye cÃ³digo para fusionar todos los archivos descargados en un Ãºnico DataFrame, que se guarda en el archivo consolidado `series_tv.pkl`. Esta funcionalidad es Ãºtil para manejar grandes datasets y asegurar que no se pierda informaciÃ³n por interrupciones en la conexiÃ³n.

## ğŸ“ Convenciones y Herramientas de Formato

- El cÃ³digo sigue el estÃ¡ndar PEP8 para estilo y buenas prÃ¡cticas en Python.
- Se utiliza **black** para el formateo automÃ¡tico del cÃ³digo.
- Se utiliza **isort** para ordenar los imports de manera consistente.
- Se utiliza **pydocstyle** para verificar la calidad y formato de los docstrings.
- Se incluyen comentarios claros y docstrings en las funciones y clases principales para facilitar la comprensiÃ³n y el mantenimiento.

---

<div align="center">

**Desarrollado con pasiÃ³n por el anÃ¡lisis de datos y el scraping web**

ğŸ“š **Universidad:** Universidad de ConcepciÃ³n

ğŸ“ **Curso:** AnÃ¡lisis de Datos

ğŸ“‹ **CÃ³digo curso:** 501350-1

ğŸ“… **Semestre:** 2025-2

</div>
